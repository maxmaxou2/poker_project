{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing d:\\poker project\\poker_project\\pokerrl_env-0.1.3-py3-none-any.whl\n",
      "Collecting numpy<2.0.0,>=1.19.5 (from pokerrl-env==0.1.3)\n",
      "  Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl.metadata (61 kB)\n",
      "Collecting pytest<7.0.0,>=6.2.4 (from pokerrl-env==0.1.3)\n",
      "  Using cached pytest-6.2.5-py3-none-any.whl.metadata (7.5 kB)\n",
      "Collecting setuptools-rust<2.0.0,>=1.5.2 (from pokerrl-env==0.1.3)\n",
      "  Using cached setuptools_rust-1.9.0-py3-none-any.whl.metadata (9.3 kB)\n",
      "Collecting attrs>=19.2.0 (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached attrs-23.2.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting iniconfig (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached iniconfig-2.0.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting packaging (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Downloading packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting pluggy<2.0,>=0.12 (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached pluggy-1.5.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting py>=1.8.2 (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached py-1.11.0-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting toml (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting atomicwrites>=1.0 (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached atomicwrites-1.4.1-py2.py3-none-any.whl\n",
      "Collecting colorama (from pytest<7.0.0,>=6.2.4->pokerrl-env==0.1.3)\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting setuptools>=62.4 (from setuptools-rust<2.0.0,>=1.5.2->pokerrl-env==0.1.3)\n",
      "  Downloading setuptools-71.1.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting semantic-version<3,>=2.8.2 (from setuptools-rust<2.0.0,>=1.5.2->pokerrl-env==0.1.3)\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting tomli>=1.2.1 (from setuptools-rust<2.0.0,>=1.5.2->pokerrl-env==0.1.3)\n",
      "  Using cached tomli-2.0.1-py3-none-any.whl.metadata (8.9 kB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-win_amd64.whl (15.8 MB)\n",
      "Using cached pytest-6.2.5-py3-none-any.whl (280 kB)\n",
      "Using cached setuptools_rust-1.9.0-py3-none-any.whl (26 kB)\n",
      "Using cached attrs-23.2.0-py3-none-any.whl (60 kB)\n",
      "Using cached pluggy-1.5.0-py3-none-any.whl (20 kB)\n",
      "Using cached py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading setuptools-71.1.0-py3-none-any.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   --------- ------------------------------ 0.6/2.3 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 24.8 MB/s eta 0:00:00\n",
      "Using cached tomli-2.0.1-py3-none-any.whl (12 kB)\n",
      "Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Using cached iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading packaging-24.1-py3-none-any.whl (53 kB)\n",
      "   ---------------------------------------- 0.0/54.0 kB ? eta -:--:--\n",
      "   ---------------------------------------- 54.0/54.0 kB 2.9 MB/s eta 0:00:00\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: tomli, toml, setuptools, semantic-version, py, pluggy, packaging, numpy, iniconfig, colorama, attrs, atomicwrites, setuptools-rust, pytest, pokerrl-env\n",
      "  Attempting uninstall: tomli\n",
      "    Found existing installation: tomli 2.0.1\n",
      "    Uninstalling tomli-2.0.1:\n",
      "      Successfully uninstalled tomli-2.0.1\n",
      "  Attempting uninstall: toml\n",
      "    Found existing installation: toml 0.10.2\n",
      "    Uninstalling toml-0.10.2:\n",
      "      Successfully uninstalled toml-0.10.2\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 63.2.0\n",
      "    Uninstalling setuptools-63.2.0:\n",
      "      Successfully uninstalled setuptools-63.2.0\n",
      "  Attempting uninstall: semantic-version\n",
      "    Found existing installation: semantic-version 2.10.0\n",
      "    Uninstalling semantic-version-2.10.0:\n",
      "      Successfully uninstalled semantic-version-2.10.0\n",
      "  Attempting uninstall: py\n",
      "    Found existing installation: py 1.11.0\n",
      "    Uninstalling py-1.11.0:\n",
      "      Successfully uninstalled py-1.11.0\n",
      "  Attempting uninstall: pluggy\n",
      "    Found existing installation: pluggy 1.5.0\n",
      "    Uninstalling pluggy-1.5.0:\n",
      "      Successfully uninstalled pluggy-1.5.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.1\n",
      "    Uninstalling packaging-23.1:\n",
      "      Successfully uninstalled packaging-23.1\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: iniconfig\n",
      "    Found existing installation: iniconfig 2.0.0\n",
      "    Uninstalling iniconfig-2.0.0:\n",
      "      Successfully uninstalled iniconfig-2.0.0\n",
      "  Attempting uninstall: colorama\n",
      "    Found existing installation: colorama 0.4.6\n",
      "    Uninstalling colorama-0.4.6:\n",
      "      Successfully uninstalled colorama-0.4.6\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 23.2.0\n",
      "    Uninstalling attrs-23.2.0:\n",
      "      Successfully uninstalled attrs-23.2.0\n",
      "  Attempting uninstall: atomicwrites\n",
      "    Found existing installation: atomicwrites 1.4.1\n",
      "    Uninstalling atomicwrites-1.4.1:\n",
      "      Successfully uninstalled atomicwrites-1.4.1\n",
      "  Attempting uninstall: setuptools-rust\n",
      "    Found existing installation: setuptools-rust 1.9.0\n",
      "    Uninstalling setuptools-rust-1.9.0:\n",
      "      Successfully uninstalled setuptools-rust-1.9.0\n",
      "  Attempting uninstall: pytest\n",
      "    Found existing installation: pytest 6.2.5\n",
      "    Uninstalling pytest-6.2.5:\n",
      "      Successfully uninstalled pytest-6.2.5\n",
      "  Attempting uninstall: pokerrl-env\n",
      "    Found existing installation: pokerrl-env 0.1.3\n",
      "    Uninstalling pokerrl-env-0.1.3:\n",
      "      Successfully uninstalled pokerrl-env-0.1.3\n",
      "Successfully installed atomicwrites-1.4.1 attrs-23.2.0 colorama-0.4.6 iniconfig-2.0.0 numpy-1.26.4 packaging-24.1 pluggy-1.5.0 pokerrl-env-0.1.3 py-1.11.0 pytest-6.2.5 semantic-version-2.10.0 setuptools-71.1.0 setuptools-rust-1.9.0 toml-0.10.2 tomli-2.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (d:\\pokemondl\\deeplearning\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (d:\\pokemondl\\deeplearning\\lib\\site-packages)\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\PokemonDL\\deepLearning\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'D:\\PokemonDL\\deepLearning\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-intel 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.4 which is incompatible.\n",
      "tensorflow-intel 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.10.0 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install ../pokerrl_env-0.1.3-py3-none-any.whl --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Could not find module 'd:\\PokemonDL\\deepLearning\\lib\\site-packages\\rusteval\\target\\release\\librusteval.dll' (or one of its dependencies). Try using the full path with constructor syntax.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01moptim\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, Game, BetLimits, GameTypes\n",
      "File \u001b[1;32md:\\PokemonDL\\deepLearning\\lib\\site-packages\\pokerrl_env\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m play_game\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mview\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m player_view, human_readable_view, json_view\n",
      "File \u001b[1;32md:\\PokemonDL\\deepLearning\\lib\\site-packages\\pokerrl_env\\play.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m return_current_player\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mview\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m player_view, human_readable_view\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransition\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m step_state,init_state\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_action\u001b[39m(action_mask,global_states,config):\n\u001b[0;32m      8\u001b[0m     readable_actions \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFOLD\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCHECK\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCALL\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\PokemonDL\\deepLearning\\lib\\site-packages\\pokerrl_env\\transition.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatatypes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PLAYER_ORDER_BY_STREET, POSITION_TO_SEAT,RAISE,CALL,FOLD,BET,CHECK, ModelActions, StateActions,Street,Player,Positions\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcardlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m encode, hand_rank\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpokerrl_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_next_player_the_aggressor, return_deck\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "File \u001b[1;32md:\\PokemonDL\\deepLearning\\lib\\site-packages\\pokerrl_env\\cardlib.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# Windows...\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     lib_path \u001b[38;5;241m=\u001b[39m path\u001b[38;5;241m.\u001b[39mjoin(release_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrusteval.dll\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 18\u001b[0m     lib \u001b[38;5;241m=\u001b[39m \u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcdll\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrelease_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlibrusteval.dll\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# takes [2,'s'] or [2,1] and returns the cactus kev encoding for that card\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(card):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ctypes\\__init__.py:452\u001b[0m, in \u001b[0;36mLibraryLoader.LoadLibrary\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    451\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[1;32m--> 452\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ctypes\\__init__.py:374\u001b[0m, in \u001b[0;36mCDLL.__init__\u001b[1;34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[0m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_FuncPtr \u001b[38;5;241m=\u001b[39m _FuncPtr\n\u001b[0;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 374\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle \u001b[38;5;241m=\u001b[39m handle\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Could not find module 'd:\\PokemonDL\\deepLearning\\lib\\site-packages\\rusteval\\target\\release\\librusteval.dll' (or one of its dependencies). Try using the full path with constructor syntax."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from pokerrl_env import Config, Game, BetLimits, GameTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PokerNN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(PokerNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "\n",
    "    def push(self, state, action):\n",
    "        if len(self.buffer) >= self.capacity:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append((state, action))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
    "        batch = [self.buffer[idx] for idx in indices]\n",
    "        state, action = zip(*batch)\n",
    "        return state, action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PokerEnv()\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "policy_net = PokerNN(state_size, action_size)\n",
    "target_net = PokerNN(state_size, action_size)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "policy_optimizer = optim.Adam(policy_net.parameters())\n",
    "sl_buffer = ReplayBuffer(10000)\n",
    "rl_buffer = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sl(batch_size):\n",
    "    if len(sl_buffer.buffer) < batch_size:\n",
    "        return\n",
    "    states, actions = sl_buffer.sample(batch_size)\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    outputs = policy_net(states)\n",
    "    loss = nn.CrossEntropyLoss()(outputs, actions)\n",
    "    loss.backward()\n",
    "    policy_optimizer.step()\n",
    "\n",
    "def train_rl(batch_size, gamma=0.99):\n",
    "    if len(rl_buffer.buffer) < batch_size:\n",
    "        return\n",
    "    states, actions, rewards, next_states, dones = rl_buffer.sample(batch_size)\n",
    "    states = torch.FloatTensor(states)\n",
    "    actions = torch.LongTensor(actions)\n",
    "    rewards = torch.FloatTensor(rewards)\n",
    "    next_states = torch.FloatTensor(next_states)\n",
    "    dones = torch.FloatTensor(dones)\n",
    "\n",
    "    q_values = policy_net(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_values = target_net(next_states).max(1)[0]\n",
    "    expected_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    loss = nn.MSELoss()(q_values, expected_q_values)\n",
    "\n",
    "    policy_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    policy_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 1000\n",
    "batch_size = 32\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = policy_net(torch.FloatTensor(state)).argmax().item()\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "        sl_buffer.push(state, action)\n",
    "        rl_buffer.push(state, action, reward, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "        train_sl(batch_size)\n",
    "        train_rl(batch_size)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "        print(f'Episode {episode} completed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
